Real biological neurons take many inputs, not just one.
Similarly, we simply combine input signals by adding them up, and the
resultant sum is the input to the sigmoid function which controls the output.
This reflects how real neurons work. The following diagram illustrates this
idea of combining inputs and then applying the threshold to the combined sum:

![sigmoid-threshold-function-diagram.jpg]

If the combined signal is not large enough then the effect of the sigmoid
threshold function is to suppress the output signal. If the sum x if large
enough the effect of the sigmoid is to fire the neuron. Interestingly, if only
one of the several inputs is large and the rest small, this may be enough to
fire the neuron. What's more, the neuron can fire if some of the inputs
are individually almost, but not quite, large enough because when combined the
signal is large enough to overcome the threshold.

![3x3-neural-network-connections.jpg]

The following diagram again shows the connected nodes, but this time a weight
is shown associated with each connection. A low weight will de-emphasise a
signal, and a high weight will amplify it.

The weight w2,3 is simply the weight associated with the signal that passed
between node 2 in a layer to node 3 in the next layer. So w1,2 is the weight
that diminishes or amplifies the signal between node 1 and node 2 in the next
layer.

![3x3-neural-network-connections-with-weights.jpg]

You might reasonably challenge this design and ask yourself why each node
should connect to every other node in the previous and next layer. They
don't have to and you could connect them in all sorts of creative ways. We
don't because the uniformity of this full connectivity is actually easier
to encode as computer instructions, and because there shouldn't be any big
harm in having a few more connections than the absolute minimum that might be
needed for solving a specific task. The learning process will de-emphasise
those few extra connections if they aren't actually needed.

What do we mean by this? It means that as the network learns to improve its
outputs by refining the link weights inside the network, some weights become
zero or close to zero. Zero, or almost zero, weights means those links
don't contribute to the network because signals don't pass. A zero
weight means the signals are multiplied by zero, which results in zero, so the
link is effectively broken.
