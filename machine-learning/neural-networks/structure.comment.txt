Real biological neurons take many inputs, not just one.
Similarly, we simply combine input signals by adding them up, and the
resultant sum is the input to the sigmoid function which controls the output.
This reflects how real neurons work. The following diagram illustrates this
idea of combining inputs and then applying the threshold to the combined sum:

![sigmoid-threshold-function-diagram.jpg]

If the combined signal is not large enough then the effect of the sigmoid
threshold function is to suppress the output signal. If the sum x is large
enough, the effect of the sigmoid is to fire the neuron. Interestingly, if only
one of the several inputs is large and the rest small, this may be enough to
fire the neuron. What's more, the neuron can fire if some of the inputs
are individually almost, but not quite, large enough because when combined the
signal is large enough to overcome the threshold.

![3x3-neural-network-connections.jpg]

The following diagram again shows the connected nodes, but this time a weight
is shown associated with each connection. A low weight will de-emphasise a
signal, and a high weight will amplify it.

The weight w2,3 is simply the weight associated with the signal that passed
between node 2 in a layer to node 3 in the next layer. So w1,2 is the weight
that diminishes or amplifies the signal between node 1 and node 2 in the next
layer.

![3x3-neural-network-connections-with-weights.jpg]

You might reasonably challenge this design and ask yourself why each node
should connect to every other node in the previous and next layer. They
don't have to and you could connect them in all sorts of creative ways. We
don't because the uniformity of this full connectivity is actually easier
to encode as computer instructions, and because there shouldn't be any big
harm in having a few more connections than the absolute minimum that might be
needed for solving a specific task. The learning process will de-emphasise
those few extra connections if they aren't actually needed.

What do we mean by this? It means that as the network learns to improve its
outputs by refining the link weights inside the network, some weights become
zero or close to zero. Zero, or almost zero, weights means those links
don't contribute to the network because signals don't pass. A zero
weight means the signals are multiplied by zero, which results in zero, so the
link is effectively broken.

we'll try doing the workings out with a smaller neural network with only 2
layers, each with 2 neurons, as shown below:

![two-layers-neural-networks-without-weights.png]

Let's imagine the two inputs are 1.0 and 0.5. The following shows these inputs
entering this smaller neural network.

![two-layers-neural-networks-with-inputs.png]

Just as before, each node turns the sum of the inputs into an output using an
activation function. We'll also use the sigmoid function that we saw before,
where x is the sum of incoming signals to a neuron, and y is the output of
that neuron.

![sigmoid-function.png]

The x in this function is the combined input into a node. That combination was
the raw outputs from the connected nodes in the previous layer, but moderated
by the link weights.

The following diagram shows all these numbers now marked.

![two-layers-neural-networks-with-weights.png]

The first layer of nodes is the input layer, and it doesn't do anything other
than represent the input signals. That is, the input nodes don't apply an
activation function to the input.

The following diagram is like the one we saw previously but now includes the
need to moderate the incoming signals with the link weights.

![node-with-input-and-weights.png]

So let's first focus on node 1 in the layer 2.

The combined moderated input is:

x = (output from first node * link weight) + (output from second node * link weight)
x = (1.0 * 0.9) + (0.5 * 0.3)
x = 0.9 + 0.15
x = 1.05

If we didn't moderate the signal, we'd have a very simple addition of the
signals 1.0 + 0.5, but we don't want that. It is the weights that do the
learning in a neural networks as they are iteratively refined to give better
and better results.

We can now, finally, calculate that node's output using the activation
function.

x = (1.0 * 0.2) + (0.5 * 0.8)
x = 0.2 + 0.4
x = 0.6

So the node's output using the sigmoid activation function would be:

y = 1/(1 + 0.5488) = 1/(1.5488)
y = 0.6457

![two-layers-neural-networks-with-outputs.png]

